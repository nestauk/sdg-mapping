{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACRONYM: A CRitical Overview of word-play in europeaN research over 30 Years tiMe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we tackle the highly important task of assessing the quality of project acronyms in Europe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "from sdg_mapping.cordis import load_cordis_projects, load_cordis_project_sdgs\n",
    "from sdg_mapping.cordis.cordis_utils import FRAMEWORK_PROGRAMMES\n",
    "from sdg_mapping.utils.sdg_utils import sdg_hex_color_codes, sdg_names\n",
    "from sdg_mapping.sdg_index.sdg_index_utils import load_sdg_index\n",
    "\n",
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from umap import UMAP\n",
    "import tensorflow_hub as hub\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise_distances, euclidean_distances\n",
    "import hashlib\n",
    "import seaborn as sns\n",
    "\n",
    "from fuzzywuzzy import process\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from string import punctuation\n",
    "import wordninja\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "fig_dir = os.path.join(project_dir, 'reports', 'analysis_cordis_sdg_index')\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = '/tmp/tfhub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []\n",
    "\n",
    "for fp in FRAMEWORK_PROGRAMMES:\n",
    "    projects.append(load_cordis_projects(fp).set_index('rcn'))\n",
    "    \n",
    "projects = pd.concat(projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df = projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total projects:', projects.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_h2020_df = load_cordis_projects('h2020').set_index('rcn')\n",
    "project_h2020_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Acronyms Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Must have an acronym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df = project_h2020_df.dropna(subset=['acronym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total projects:', project_h2020_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't count acronyms with fewer than 3 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(project_h2020_df['acronym'].str.len(), bins=40)\n",
    "ax.set_xlabel('Acronym Length')\n",
    "ax.set_ylabel('Frequency');\n",
    "# ax.axvline(2, color='C3', linestyle='--')\n",
    "# ax.axvline(12, color='C3', linestyle='--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df = project_h2020_df[project_h2020_df['acronym'].str.len() > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Regular Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many of the so-called project acronyms are in fact just regular names. To count as a true acronym the token must have been supplied in full upper case by the applicant. No other casing is permitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_h2020_df = project_h2020_df[project_h2020_df['acronym'].str.isupper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_upper(acronym):\n",
    "    uppers = len([a for a in acronym if a.isupper()])\n",
    "    return uppers / len(acronym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df['percent_upper'] = project_h2020_df['acronym'].apply(percent_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "project_h2020_df['percent_upper'].plot.hist(cumulative=True, bins=100, density='normed', histtype='step', ax=ax)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel('Upper Case Fraction')\n",
    "ax.set_ylabel('Cumulative Frequency (Norm)')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df['percent_upper'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total projects with >50% upper case:', project_h2020_df[project_h2020_df['percent_upper'] > 0.5].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another criteria for acronyms is that all of the letters in the acronym are present in upper case in the title. This rules out instances where the apparent acronym is in fact simply a word from the title. For example the project with the acronym _STRUCTURALISM_ and title _The Roots of Mathematical Structuralism_ does not count as a true acronym because the acronym itself appears as a complete token in the title. On the other hand the title of the _SLATE_ project, *Submarine LAndslides and Their impact on Europe*, contains the letters of the acronym spread across multiple tokens. We consider this to satisfy the condition.\n",
    "\n",
    "To check whether the acronym is indeed a true acronym of the title text, we check that the title contains sufficient upper case characters to form the acronym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_acronym(acronym, title):\n",
    "    title = ''.join(t for t in title if t not in punctuation)\n",
    "    acronym = ''.join(a for a in acronym if a not in punctuation)\n",
    "    acronym = ''.join(a for a in acronym if not a.isdigit())\n",
    "    title = title.replace(acronym, '')\n",
    "    title = title.upper()\n",
    "    r = '.*'.join(acronym.upper())\n",
    "    is_in = re.findall(r, title)\n",
    "    if len(is_in) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_upper_case_match(acronym, title):\n",
    "    t_counts = Counter(title)\n",
    "    acronym = ''.join(a for a in acronym if a.isupper())\n",
    "    a_counts = Counter(acronym)\n",
    "    \n",
    "    counts = []\n",
    "    for key, a_count in a_counts.items():\n",
    "        t_count = t_counts[key]\n",
    "        if t_count <= a_count:\n",
    "            counts.append(t_count)\n",
    "        elif t_count > a_count:\n",
    "            counts.append(a_count)\n",
    "    return np.sum(counts) / len(acronym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = (project_h2020_df.apply(lambda row: percentage_upper_case_match(row['acronym'], row['title']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(u, bins=20, cumulative=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contains_acronym(acronym, title, min_ratio=80):\n",
    "#     title = ''.join([t for t in title if t not in punctuation])\n",
    "#     acronym = ''.join([a for a in acronym if t not in punctuation])\n",
    "#     title = title.replace(acronym, '')\n",
    "#     title_caps = ''.join([t for t in title if (t.isupper()) & (t in acronym)])\n",
    "#     ratio = fuzz.ratio(acronym, title_caps)\n",
    "#     if ratio >= min_ratio:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df['contains_acronym'] = (project_h2020_df\n",
    "                                  .apply(lambda row: contains_acronym(row['acronym'], row['title']), axis=1))\n",
    "project_h2020_df = project_h2020_df[project_h2020_df['contains_acronym']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substring Cheating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we want the title to contain the requisite letters to form the supposed acronym, we also do not want the acronym (or substantial parts of it) to appear wholesale in the title.\n",
    "\n",
    "**This is hard because many projects contain the acronym in the title as a clarification.** It's a bit of an edge case, so maybe we can leave it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_honest(acronym, title, max_fuzz=80):\n",
    "    for stub in ['{}:', '{} -', '({})', '( {} )', '{}-', '{} :', '- {}', ': {}', '[{}]']:\n",
    "        title.replace(stub.format(acronym), '')\n",
    "    title_doc = nlp(title)\n",
    "    title_tokens = [t.lower_ for t in title_doc]\n",
    "    title_doc = [t.lower_ for t in title_doc if len(t) > 2]\n",
    "    fuzzes = process.extract(acronym.lower(), title_doc)\n",
    "    if any([True if f[1] > max_fuzz else False for f in fuzzes]):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df['is_honest'] = (project_h2020_df\n",
    "                                  .apply(lambda row: is_honest(row['acronym'], row['title']), axis=1))\n",
    "project_h2020_df = project_h2020_df[project_h2020_df['is_honest']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Cheaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many titles which are in all or mostly upper case. These people are clearly trying their luck, hoping that whatever acronym they have chosen will fortuitously arise from the characters in their title. A true acronym must be created with intention, so these are to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_upper(text):\n",
    "    return np.sum([t.isupper() for t in text]) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_percent_upper = project_h2020_df['title'].apply(percent_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(title_percent_upper, bins=50)\n",
    "ax.set_xlabel('Fraction Upper Case')\n",
    "ax.set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_thresh = .6\n",
    "project_h2020_df = project_h2020_df[title_percent_upper <= max_thresh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that we are picking up indirect semantic relationships rather than simply assessing whether the acronym is present in the text itself. We will remove any substring of the objective text that is equivalent to the acronym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_acronym(acronym, text):\n",
    "#     return (text\n",
    "#             .replace(acronym, '')\n",
    "#             .replace(acronym.title(), '')\n",
    "#             .replace(acronym.upper(), '')\n",
    "#             .replace(acronym.lower(), '')\n",
    "#            )\n",
    "\n",
    "def remove_acronym(acronym, text, threshold=80):\n",
    "    doc = nlp.tokenizer(text)\n",
    "    tokens = set([t.lower_ for t in doc])\n",
    "    choices = process.extract(acronym.lower(), tokens, limit=10)\n",
    "    removes = [c[0] for c in choices if c[1] >= threshold]\n",
    "    words = [t.text for t in doc if t.lower_ not in removes]\n",
    "    doc2 = Doc(doc.vocab, words=words)\n",
    "    return doc2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df['text_mod'] = (project_h2020_df\n",
    "                                .apply(lambda row: remove_acronym(row['acronym'], row['objective']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_h2020_df = project_h2020_df[~project_h2020_df['acronym'].str.contains(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "# path = hashlib.sha1(module_url.encode(\"utf8\")).hexdigest()\n",
    "model = hub.load('/Users/grichardson/models/universal-sentence-encoder_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "        \n",
    "objective_embeddings = []\n",
    "for chunk in chunks(project_h2020_df['text_mod'], 1000):\n",
    "    objective_embeddings.append(model(chunk).numpy())\n",
    "    \n",
    "objective_embeddings = np.concatenate(objective_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronym_embeddings = model(project_h2020_df['acronym'])\n",
    "acronym_embeddings = acronym_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "for ac, ob in zip(acronym_embeddings, objective_embeddings):\n",
    "    dists.append(cosine(ac, ob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df['dists'] = dists\n",
    "project_h2020_df['sim'] = -1 * (project_h2020_df['dists'] -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(dists, bins=50)\n",
    "ax.set_xlabel('Cosine Distance')\n",
    "ax.set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(project_h2020_df['sim'], bins=50)\n",
    "ax.set_xlabel('Cosine Similarity')\n",
    "ax.set_ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BestAcronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting by distance we can see the acronyms that are most similar to the text of the project objective (without themselves appearing in the text).\n",
    "\n",
    "Some good examples include:\n",
    "\n",
    "- TECTONIC: The physics of Earthquake faulting: learning from laboratory earthquake prediCTiON to Improve forecasts of the spectrum of tectoniC failure modes\n",
    "- ORCA: Optimizing Research tools for Cetaceans in Archaeology\n",
    "- GATTACA: Genetics of Alternative Transcript Abundance upon immune Cellular Activation\n",
    "- MAGMA: Melting And Geodynamic Models of Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df.sort_values('dists')[['acronym', 'title']][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst Acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the other end of the spectrum, we have acronyms that have little semantic relationship to the objectives of the project. These include acronyms that aren't real words (or common acronyms for phrases) and common words that are found generally across many topics.\n",
    "\n",
    "- IMPRESS: IMproving Preparedness and Response of HEalth Services in major crise\n",
    "- SMOOTH: SMart rObOTs for fire-figHting\n",
    "- HMCS: Handheld Molecular Contaminant Screener\n",
    "- AWESOME: Advanced Wind Energy Systems Operation and Maintenance Expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(project_h2020_df\n",
    " .sort_values('dists', ascending=False)[['acronym', 'title']][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronym-Objective Semantic Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UMAP()\n",
    "umap_ac_vecs = umap.fit_transform(acronym_embeddings)\n",
    "umap_ob_vecs = umap.fit_transform(objective_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.scatter(umap_ac_vecs[:, 1], umap_ac_vecs[:, 0], alpha=.02)\n",
    "ax.scatter(umap_ob_vecs[:, 1], umap_ob_vecs[:, 0], alpha=.02)\n",
    "ax.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronym Length and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df['acronym_length'] = project_h2020_df['acronym'].str.len()\n",
    "project_h2020_df = project_h2020_df[project_h2020_df['acronym_length'] < 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 3.5))\n",
    "sns.boxplot(data=project_h2020_df, x='acronym_length', y='sim', color='C0', ax=axs[0])\n",
    "mean_length_sims = project_h2020_df.groupby('acronym_length')['sim'].mean()\n",
    "axs[0].scatter(range(len(mean_length_sims)), mean_length_sims, zorder=5, color='white')\n",
    "axs[0].set_xlabel('Acronym Length')\n",
    "axs[0].set_ylabel('Acronym-Objective Similarity')\n",
    "axs[1].hist(project_h2020_df['acronym_length'], bins=8)\n",
    "axs[1].set_xlabel('Acronym Length')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best acronyms are made exclusively from letters that appear only as the first character of words in the title. We calculate the fraction of letters in each aconym that meet this criteria as another metric of acronym quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_share(acronym, text):\n",
    "    count = []\n",
    "    i = 0\n",
    "    words = text.split(' ')\n",
    "    for a in acronym:\n",
    "        for j, word in enumerate(words):\n",
    "            if word.startswith(a):\n",
    "                count.append(1)\n",
    "                words = words[j+1:]\n",
    "                break\n",
    "    return np.sum(count) / len(acronym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_share_score = project_h2020_df.apply(lambda row: start_share(row['acronym'], row['title']), axis=1)\n",
    "project_h2020_df['start_share'] = start_share_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(project_h2020_df['start_share'], bins=100, cumulative=True, histtype='step', linewidth=2, density='normed')\n",
    "ax.set_xlabel('Title Acronymity')\n",
    "ax.set_ylabel('Cumulative Frequency (Norm)')\n",
    "ax.set_xlim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly we see that over 70% of projects have an acronym that satisfies the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(project_h2020_df['start_share'], project_h2020_df['sim'], alpha=.1)\n",
    "ax.set_xlabel('Title Acronymity')\n",
    "ax.set_ylabel('Acronym-Objective Similarity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see virtually no trend in the relationship between title acronymity and the text similarity. However it does permit us to define a new selecton criteria for the best acronyms - those which maximise both metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronym-Objective Similarity Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df = project_h2020_df[(project_h2020_df['start_date'].dt.year <= 2020) \n",
    "                                    & (project_h2020_df['start_date'].dt.year >= 1990)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouper = pd.Grouper(freq='Y', key='start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "m = project_h2020_df.groupby(grouper)['sim'].mean().rolling(3).mean()\n",
    "m.plot(linewidth=4)\n",
    "s = project_h2020_df.groupby(grouper)['sim'].std().rolling(3).mean()\n",
    "(s + m).plot(color='C0')\n",
    "(m - s).plot(color='C0')\n",
    "ax.set_xlabel('Start Year')\n",
    "ax.set_ylabel('Acronym-Objective Similarity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "years = []\n",
    "for year, group in project_h2020_df.groupby(grouper)['acronym']:\n",
    "    lens.append(group.str.len().mean())\n",
    "    years.append(year)\n",
    "    \n",
    "length_time = pd.Series(data=lens, index=years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronym Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "m = length_time.rolling(3).mean()\n",
    "std = length_time.rolling(3).mean()\n",
    "# ax.errorbar(m.index, m, yerr=std)\n",
    "ax.plot(m)\n",
    "ax.set_xlabel('Start Year')\n",
    "ax.set_ylabel('Mean Acronym Length')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraction of Projects with Acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronym_counts = project_h2020_df.groupby(grouper)['dists'].count()\n",
    "projects = projects[(projects['start_date'].dt.year <= 2020) \n",
    "                    & (projects['start_date'].dt.year >= 1990)]\n",
    "project_counts = projects.groupby(grouper)['title'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acronym_frac_time = (acronym_counts / project_counts) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "acronym_frac_time.rolling(3).mean().plot(ax=ax)\n",
    "ax.set_xlabel('Start Year')\n",
    "ax.set_ylabel('Projects with Acronym (%)')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronyms by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eu_country_codes():\n",
    "    country_df = pd.read_json(f'{data_path}/raw/countries/countries_restcountries_api.json')\n",
    "    europe = []\n",
    "    for code, c in zip(country_df['alpha2Code'], country_df['regionalBlocs']):\n",
    "        for x in c:\n",
    "            if x['acronym'] == 'EU':\n",
    "                europe.append(code)\n",
    "    \n",
    "    # Britain called 'UK' in CORDIS\n",
    "    europe = sorted(['UK' if e == 'GB' else e for e in europe])\n",
    "    return europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe = generate_eu_country_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "project_h2020_df.groupby('coordinator_country')['sim'].mean().reindex(europe).dropna().sort_values().plot.barh(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "(c / projects.groupby('coordinator_country')['title'].count().reindex(europe).dropna().sort_values() * 100).sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the amount of funding correspond to the quality of the acronym?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amounts = []\n",
    "ids = []\n",
    "for call, group in project_h2020_df.groupby('call'):\n",
    "    if group.shape[0] >= 25:\n",
    "        std = group['ec_max_contribution'].std()\n",
    "        mean = group['ec_max_contribution'].mean()\n",
    "        if std > 0:\n",
    "            a = (group['ec_max_contribution'] - mean) / std\n",
    "            ids.extend(group.index.values)\n",
    "            amounts.extend(a)\n",
    "\n",
    "# acro_fund_df = pd.DataFrame(data={'en': eng, 'non_en': non_eng})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amounts_normed = pd.Series(amounts, index=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(project_h2020_df.loc[amounts_normed.index]['sim'], amounts_normed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does English Score Higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_uk = enchant.Dict(\"en_UK\")\n",
    "en_us = enchant.Dict(\"en_US\")\n",
    "fr = enchant.Dict(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(acronym):\n",
    "    if ' ' in acronym:\n",
    "        acronym = acronym.split(' ')\n",
    "    else:\n",
    "        acronym = acronym.split('-')\n",
    "    for a in acronym:\n",
    "        if en_uk.check(a):\n",
    "            return True\n",
    "        elif en_uk.check(a):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_h2020_df['is_english'] = project_h2020_df['acronym'].apply(is_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = []\n",
    "non_eng = []\n",
    "for call, group in project_h2020_df.groupby('call'):\n",
    "    if group.shape[0] >= 25:\n",
    "        std = group['ec_max_contribution'].std()\n",
    "        mean = group['ec_max_contribution'].mean()\n",
    "        if std > 0:\n",
    "            amounts = (group['ec_max_contribution'] - mean) / std\n",
    "            eng.extend(amounts[group['is_english']])\n",
    "            non_eng.extend(amounts[~group['is_english']])\n",
    "\n",
    "eng = np.array(eng)\n",
    "eng = eng[~pd.isnull(eng)]\n",
    "non_eng = np.array(non_eng)\n",
    "non_eng = non_eng[~pd.isnull(non_eng)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(eng, bins=200, cumulative=True, density='normed', histtype='step')\n",
    "ax.hist(non_eng, bins=200, cumulative=True, density='normed', histtype='step')\n",
    "ax.set_xlim(-3, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(eng, non_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspiration Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Query google word2vec vectors for 30 most similar terms to each acronym\n",
    "2. Use queries to create edgelist and build a network\n",
    "3. Maybe threshold edges\n",
    "4. Community detection\n",
    "5. Use tf-hub model to calculate vector for the community\n",
    "6. Look at distance between abstract text and community of acronym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'DOG'\n",
    "s = 'Doing Odd Gurns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = '.*'.join(a)\n",
    "m = re.match(r, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load_word2vec_format('/Users/grichardson/nesta/manifesto/data/external/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(t):\n",
    "    try:\n",
    "        vec = w2v.get_vector(t)\n",
    "        return vec\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_possible_acronyms(title, search_topn, return_topn=None, max_fuzz=80):\n",
    "\n",
    "    title_doc = nlp(title)\n",
    "    title_tokens = [t.lower_ for t in title_doc]\n",
    "    title_doc = [t.lower_ for t in title_doc if len(t) > 2]\n",
    "    title_vecs = []\n",
    "    for t in title_doc:\n",
    "        vec = get_vector(t)\n",
    "        if vec is not None:\n",
    "            title_vecs.append(vec)\n",
    "    title_vecs = np.array(title_vecs)\n",
    "    doc_vec = np.mean(title_vecs, axis=0)\n",
    "    close_matches = w2v.similar_by_vector(doc_vec, topn=search_topn)\n",
    "    close_matches = set(chain(*[[t.lower() for t in m[0].split('_') if len(t) > 3] for m in close_matches]))\n",
    "\n",
    "    acronyms = []\n",
    "    sims = []\n",
    "    for candidate in close_matches:\n",
    "        if '#' in candidate:\n",
    "            continue\n",
    "        fuzzes = process.extract(candidate, title_tokens)\n",
    "        if any([True if f[1] > max_fuzz else False for f in fuzzes]):\n",
    "            continue\n",
    "\n",
    "        r = '.*'.join(candidate)\n",
    "        is_in = re.findall(r, title.lower())\n",
    "        if len(is_in) > 0:\n",
    "            candidate_vec = get_vector(candidate)\n",
    "            if candidate_vec is not None:\n",
    "                acronyms.append(candidate)\n",
    "                sims.append(1 - cosine(candidate_vec, doc_vec))\n",
    "    acronyms = [(a, f'{s:.2f}') for s, a in sorted(zip(sims, acronyms), reverse=True)]\n",
    "\n",
    "    return acronyms[:return_topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = project_h2020_df.sample(10).title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in titles:\n",
    "    print(title)\n",
    "    for result in find_possible_acronyms(title, 400, 10):\n",
    "        print(result)\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
