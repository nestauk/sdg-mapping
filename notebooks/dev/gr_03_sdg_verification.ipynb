{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sdg_mapping.cordis import load_cordis_projects, load_cordis_project_sdgs\n",
    "from sdg_mapping.cordis.cordis_utils import FRAMEWORK_PROGRAMMES\n",
    "from sdg_mapping.utils.sdg_utils import sdg_hex_color_codes, sdg_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CORDIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []\n",
    "project_sdgs = []\n",
    "project_sdg_probs = []\n",
    "\n",
    "for fp in ['fp6', 'fp7', 'h2020']:\n",
    "    projects.append(load_cordis_projects(fp).set_index('rcn'))\n",
    "    project_sdgs.append(load_cordis_project_sdgs(fp, 'strict_label').set_index('rcn'))\n",
    "    project_sdg_probs.append(load_cordis_project_sdgs(fp, 'probability').set_index('rcn'))\n",
    "    \n",
    "projects = pd.concat(projects)\n",
    "project_sdgs = pd.concat(project_sdgs)\n",
    "project_sdg_probs = pd.concat(project_sdg_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_dir = f'{data_path}/interim/doccano/results'\n",
    "label_dir = f'{data_path}/interim/doccano/results/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for file in os.listdir(annotated_dir):\n",
    "    if '.csv' in file:\n",
    "        fin = os.path.join(annotated_dir, file)\n",
    "        df = pd.read_csv(fin)\n",
    "        n = int(fin.split('_')[-1].split('.')[0][3:])\n",
    "\n",
    "        label_path = os.path.join(label_dir, f'labels_sdg{n}.json')\n",
    "        labels = pd.read_json(label_path)\n",
    "        label_map = {i: k for i, k in zip(labels['id'], labels['suffix_key'])}\n",
    "        df['label'] = df['label'].map(label_map)\n",
    "        df['label'] = df['label'].map({'y': 1, 'n': 0})\n",
    "\n",
    "        df = df.rename(columns={'meta.rcn': 'rcn'})\n",
    "        df = df.set_index('rcn')\n",
    "\n",
    "        dfs[n] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Classification Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for sdg in range(1, 17):\n",
    "    df_score = project_sdgs.merge(dfs[sdg], left_index=True, right_index=True, how='right')\n",
    "    score = precision_recall_fscore_support(df_score['label'], df_score[sdg])\n",
    "    scores.append(np.array(score).T.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(np.array(scores))\n",
    "score_df.columns =[\n",
    "    '0_precision', '0_recall', '0_f1', '0_support',\n",
    "    '1_precision', '1_recall', '1_f1', '1_support',\n",
    "]\n",
    "score_df.index = range(1,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "_score_df = score_df[[c for c in score_df.columns if 'support' not in c]]\n",
    "_score_df = _score_df.sort_values(by='1_f1', ascending=False)\n",
    "_score_df.index = [sdg_names()[i] for i in _score_df.index.values]\n",
    "sns.heatmap(_score_df, fmt='.2f', annot=True, cmap='RdBu')\n",
    "ax.axvline(3, color='#ffffff', linewidth=3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../reports/eda/figures/cordis_precision_recall_f1_heatmap.png', dpi=300);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on F1 scores for positive samples:\n",
    "\n",
    "- Performing well: 2, 3, 6, 7, 11\n",
    "- Performing moderately: 8, 16\n",
    "- Performing badly: 1, 4, 5, 9, 10, 12, 13, 14, 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out titles of misclassified projects (FNs and FPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for sdg in range(1, 17):\n",
    "    print('===', sdg_names()[sdg], '===')\n",
    "    _df = project_sdgs.merge(dfs[sdg], left_index=True, right_index=True, how='right')\n",
    "    _df['prob'] = project_sdg_probs.loc[_df.index.values][sdg]\n",
    "    df_fp = _df[(_df[sdg] == 1) & (_df['label'] == 0)]\n",
    "    df_fn = _df[(_df[sdg] == 0) & (_df['label'] == 1)]\n",
    "    print('False Positives')\n",
    "    for v, s in zip(df_fp['text'].values[:5], df_fp['prob'].values[:5]):\n",
    "        print(f'> {s:.2f}', v.split('=== ')[1])\n",
    "    print('\\nFalse Negatives')\n",
    "    if df_fn.shape[0] > 0:\n",
    "        for v, s in zip(df_fn['text'].values[:5], df_fn['prob'].values[:5]):\n",
    "            print(f'> {s:.2f}', v.split('=== ')[1])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
