{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sdg_mapping.cordis import load_cordis_projects, load_cordis_project_sdgs\n",
    "from sdg_mapping.cordis.cordis_utils import FRAMEWORK_PROGRAMMES\n",
    "from sdg_mapping.utils.sdg_utils import sdg_hex_color_codes, sdg_names\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_keys = list(range(1, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []\n",
    "project_sdgs = []\n",
    "\n",
    "for fp in FRAMEWORK_PROGRAMMES:\n",
    "    projects.append(load_cordis_projects(fp).set_index('rcn'))\n",
    "    project_sdgs.append(load_cordis_project_sdgs(fp, 'probability').set_index('rcn'))\n",
    "    \n",
    "projects = pd.concat(projects)\n",
    "project_sdgs = pd.concat(project_sdgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = projects.merge(project_sdgs, left_index=True, right_index=True, how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = f'{data_path}/raw/sdg_vocabulary/siris_sdg_vocabulary_v1.2.xlsx' \n",
    "\n",
    "sdg_vocab = {}\n",
    "\n",
    "for i in range(1, 17):\n",
    "    df = pd.read_excel(vocab_path, sheet_name=f'SDG {i}')\n",
    "    sdg_vocab[i] = df['keyword'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_dir = f'{data_path}/interim/doccano/results'\n",
    "label_dir = f'{data_path}/interim/doccano/results/labels'\n",
    "\n",
    "dfs = {}\n",
    "for file in os.listdir(annotated_dir):\n",
    "    if '.csv' in file:\n",
    "        fin = os.path.join(annotated_dir, file)\n",
    "        df = pd.read_csv(fin)\n",
    "        n = int(fin.split('_')[-1].split('.')[0][3:])\n",
    "\n",
    "        label_path = os.path.join(label_dir, f'labels_sdg{n}.json')\n",
    "        labels = pd.read_json(label_path)\n",
    "        label_map = {i: k for i, k in zip(labels['id'], labels['suffix_key'])}\n",
    "        df['label'] = df['label'].map(label_map)\n",
    "        df['label'] = df['label'].map({'y': 1, 'n': 0})\n",
    "\n",
    "        df = df.rename(columns={'meta.rcn': 'rcn'})\n",
    "        df = df.set_index('rcn')\n",
    "\n",
    "        dfs[n] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs.values():\n",
    "    projects = projects.drop(projects.index.intersection(df.index.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, nrows=4, figsize=(15,13))\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    projects[i+1].plot.hist(ax=ax)\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Documents from Across the Prediction Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_keys = list(range(1, 17))\n",
    "\n",
    "project_ids = {}\n",
    "sizes = [100, 100, 100, 300]\n",
    "\n",
    "for sdg in sdg_keys:\n",
    "    probs = projects[sdg]\n",
    "    \n",
    "    steps = np.linspace(probs.min(), probs.max(), 5)\n",
    "    ids = []\n",
    "    for lower, upper, size in zip(steps[:-1], steps[1:], sizes):\n",
    "        probs_step = probs[(probs > lower) & (probs <= upper)]\n",
    "        if probs_step.shape[0] < size:\n",
    "            ids.extend(probs_step.index.values)\n",
    "        else:\n",
    "            ids_q = probs[(probs > lower) & (probs <= upper)].sample(size, random_state=0).index.values\n",
    "            ids.extend(ids_q)\n",
    "    project_ids[sdg] = ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Similar Documents that Are Not Positively Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load('/Users/grichardson/models/universal-sentence-encoder_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for chunk in chunks(projects['objective'].fillna(''), 1000):\n",
    "    embeddings.extend(embed(chunk).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(np.array(embeddings))\n",
    "vec_df.index = projects.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AnnoyIndex(512, 'angular')\n",
    "\n",
    "for i, v in zip(projects.index.values, np.array(vec_df)):\n",
    "    t.add_item(i, v)\n",
    "    \n",
    "t.build(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vecs = []\n",
    "\n",
    "for sdg in sdg_keys:\n",
    "    ids = projects.sort_values(sdg, ascending=False).index.values[:100]\n",
    "    mean_vecs.append(vec_df.loc[ids].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_ids = {}\n",
    "\n",
    "for sdg, vec in zip(sdg_keys, mean_vecs):\n",
    "    similar = t.get_nns_by_vector(vec, 3000)\n",
    "    similar_negative_projects = projects.loc[similar][projects.loc[similar][sdg] < .5][sdg]\n",
    "    similar_negative_projects = similar_negative_projects.sort_values(ascending=False)\n",
    "    \n",
    "    similar_negative_ids = set(similar_negative_projects.index.values[:200])\n",
    "    \n",
    "    sdg_ids = set(project_ids[sdg])\n",
    "    \n",
    "    extra_ids[sdg] = list(similar_negative_ids.difference(sdg_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in project_ids.items():\n",
    "    project_ids[k].extend(extra_ids[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents That are Similar to Manually Labelled Documents but Not Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs.values():\n",
    "    df['text'] = df['text'].apply(lambda x: x.split(' === ')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correctly_labelled_vec = {}\n",
    "\n",
    "# for sdg, df in dfs.items():\n",
    "#     positives = df[df['label'] == 1]\n",
    "#     embedding = embed(df['text'].values).numpy().mean(axis=0)\n",
    "#     correctly_labelled_vec[sdg] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_ids = {}\n",
    "\n",
    "# for sdg, vec in zip(sdg_keys, correctly_labelled_vec.values()):\n",
    "#     similar = t.get_nns_by_vector(vec, 201)\n",
    "#     similar_negative_ids = set(similar)\n",
    "#     sdg_ids = set(project_ids[sdg])\n",
    "    \n",
    "#     extra_ids[sdg] = list(similar_negative_ids.difference(sdg_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in project_ids.items():\n",
    "    project_ids[k].extend(extra_ids[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_labelled_vec = {}\n",
    "\n",
    "similar_negative_ids = {}\n",
    "\n",
    "for sdg, df in dfs.items():\n",
    "    positives = df[df['label'] == 1]\n",
    "#     embedding = embed(df['text'].values).numpy().mean(axis=0)\n",
    "    embedding = embed(df['text'].values).numpy()\n",
    "    similar = []\n",
    "    for vec in embedding:\n",
    "        similar.extend(t.get_nns_by_vector(vec, 20)[1:])\n",
    "    similar_negative_ids[sdg] = set(similar)\n",
    "#     correctly_labelled_vec[sdg] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in project_ids.items():\n",
    "    extra_ids = list(similar_negative_ids[k].difference(sdg_ids))\n",
    "    project_ids[k].extend(extra_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_counts(doc, matcher):\n",
    "    matches = matcher(doc)\n",
    "    id_counts = Counter([m[0] for m in matches])\n",
    "    return id_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_sdgs(texts, ids, matcher, batch_size=1000, n_process=3, disable=[]):\n",
    "    match_counts = []\n",
    "    for doc in nlp.pipe(texts, batch_size=batch_size, \n",
    "                        disable=disable, n_process=n_process):\n",
    "        match_counts.append(get_match_counts(doc, matcher))\n",
    "    df = pd.DataFrame(match_counts, index=ids)\n",
    "    df.columns = [nlp.vocab[c].text for c in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sdg_matches(doc):\n",
    "    x = []\n",
    "    for sent in doc.sents:\n",
    "        matches = matcher(sent.as_doc())\n",
    "        x.extend([(nlp.vocab[m[0]].text, sent[m[1]: m[2]]) for m in matches])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_vocab_extra = {}\n",
    "\n",
    "for sdg in range(1, 17):\n",
    "    kw_counts = Counter(\n",
    "        chain(*[kw.split(' ') for kw in sdg_vocab[sdg]])\n",
    "    ).most_common(20)\n",
    "    extra_kw = [k for k, v in kw_counts if (len(k) > 3) & (v > 2)]\n",
    "    sdg_vocab_extra[sdg] = extra_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removes = {\n",
    "    1: ['social', 'development', 'government', 'work'],\n",
    "    2: ['work', 'production, growth', 'productivity', 'income', 'quality'],\n",
    "    3: ['carbon', 'child', 'quality'],\n",
    "    4: ['discrimination', 'social', 'global', 'prejudice'],\n",
    "    5: ['work', 'against', 'convention'],\n",
    "    6: ['management', 'pollution', 'system', 'quality'],\n",
    "    7: [],\n",
    "    8: ['child', 'access'],\n",
    "    9: ['development', 'research', 'social', 'sustainable', 'work'],\n",
    "    10: ['social', 'country', 'financial', 'work'],\n",
    "    11: ['design', 'climate', 'change', 'environmental'],\n",
    "    12: ['sustainable'],\n",
    "    13: ['change', 'management', 'global', 'sustainable'],\n",
    "    14: ['sustainable', 'resource'],\n",
    "    15: ['loss'],\n",
    "    16: ['right', 'child', 'education', 'social', 'access']\n",
    "\n",
    "}\n",
    "\n",
    "adds = {\n",
    "    1: ['welfare', 'poverty'],\n",
    "    2: ['hunger', 'malnutrition', 'nutrition'],\n",
    "    3: [],\n",
    "    4: ['childcare'],\n",
    "    5: ['bride', 'female'],\n",
    "    6: ['hygiene'],\n",
    "    7: ['battery', 'photovoltaic'],\n",
    "    8: [],\n",
    "    9: [],\n",
    "    10: ['lgbt', 'lgbtq', 'lgbtq+', 'homophobia', 'lesbian', 'gay', 'bisexual', 'transgender', 'intersex'],\n",
    "    11: [],\n",
    "    12: ['plastic'],\n",
    "    13: [],\n",
    "    14: ['sea'],\n",
    "    15: [],\n",
    "    16: ['corrupt', 'justice']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sdg, v in sdg_vocab_extra.items():\n",
    "    remove = removes[sdg]\n",
    "    add = adds.get(sdg, [])\n",
    "    \n",
    "    sdg_vocab_extra[sdg] = [s for s in sdg_vocab_extra[sdg] if s not in remove]\n",
    "    sdg_vocab_extra[sdg].extend(add)\n",
    "    \n",
    "    sdg_vocab_extra[sdg] = list(set(sdg_vocab_extra[sdg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
    "\n",
    "for sdg in range(1, 17):\n",
    "    matcher.add(f\"sdg{sdg}\", None, *[nlp(kw) for kw in sdg_vocab[sdg]])\n",
    "    matcher.add(f\"sdg{sdg}\", None, *[nlp(kw) for kw in sdg_vocab_extra[sdg]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f get_match_sdgs \n",
    "df_kws = get_match_sdgs(\n",
    "    projects['objective'].fillna(''), \n",
    "    projects.index.values, matcher, \n",
    "    disable=['ner', 'parser']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_match_ids = {}\n",
    "\n",
    "for col in df_kws.columns:\n",
    "    sdg = int(col.replace('sdg', ''))\n",
    "    \n",
    "    ids = df_kws.sort_values(col, ascending=False).index.values[:600]\n",
    "    \n",
    "    sdg_ids = set(project_ids[sdg])\n",
    "    \n",
    "    phrase_match_ids[sdg] = list(set(ids).difference(sdg_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in project_ids.items():\n",
    "    project_ids[k].extend(phrase_match_ids[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in some extra random articles to add balance to the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 19\n",
    "\n",
    "((n * 2) - 20) / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in projects.loc[phrase_match_ids[16]].sample(20).title.values:\n",
    "    print('>', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_adjustment = {\n",
    "    1: 0,\n",
    "    2: 0.6,\n",
    "    3: 0.6,\n",
    "    4: 0,\n",
    "    5: 0,\n",
    "    6: 0.4,\n",
    "    7: 0.5,\n",
    "    8: 0,\n",
    "    9: 0,\n",
    "    10: 0,\n",
    "    11: 0.4,\n",
    "    12: 0.3,\n",
    "    13: 0.6,\n",
    "    14: 0.3,\n",
    "    15: 0.9,\n",
    "    16: 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sdg, v in negative_adjustment.items():\n",
    "    n = round(len(project_ids[k]) * v)\n",
    "    neg_ids = set(projects.sample(n, random_state=0).index.values)\n",
    "    neg_ids = neg_ids.difference(project_ids[k])\n",
    "    \n",
    "    project_ids[sdg].extend(neg_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exports = {}\n",
    "\n",
    "for sdg in sdg_keys:\n",
    "    export = projects.loc[project_ids[sdg]].sample(frac=1, random_state=0)\n",
    "    exports[sdg] = export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sdg in sdg_keys:\n",
    "    test = exports[sdg]\n",
    "    test['text'] = '===== ' + test['title'] + ' =====                    ' + test['objective']\n",
    "\n",
    "    test = test.reset_index()\n",
    "    test = test[['rcn', 'text', 1]]\n",
    "    test = test.rename(columns={'rcn': 'ID', 'text': 'Text', 1: 'Label'})\n",
    "\n",
    "    test['Label'] = (test['Label'] > .5).map({True: 'Yes', False: 'No'})\n",
    "    test.dropna(inplace=True)\n",
    "    \n",
    "    test.to_csv(f'../../data/interim/smart_sdg_{sdg}.csv', index=False)\n",
    "    \n",
    "    test['Label'] = ''\n",
    "    test.to_csv(f'../../data/interim/smart_sdg_{sdg}_unlabelled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matcher (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "patterns = defaultdict(list)\n",
    "\n",
    "for sdg in range(1, 17):\n",
    "    kws = sdg_vocab[sdg]\n",
    "\n",
    "    for k in kws:\n",
    "        _k = k.split(' ')\n",
    "        if len(_k) == 2:\n",
    "            for i in range(0, n + 1):\n",
    "                p = [{\"LEMMA\": _k[0]}, *[{}] * i, {\"LEMMA\": _k[1]}]\n",
    "                p_r = [{\"LEMMA\": _k[1]}, *[{}] * i, {\"LEMMA\": _k[0]}]\n",
    "                patterns[sdg].append(p)\n",
    "                patterns[sdg].append(p_r)\n",
    "        else:\n",
    "            patterns[sdg].append([{\"LEMMA\": l} for l in _k])\n",
    "            \n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for sdg in range(1, 17):\n",
    "    matcher.add(f'sdg{sdg}', None, *patterns[sdg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f get_match_counts get_match_counts(doc, matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f get_match_sdgs get_match_sdgs(projects['objective'].values[:100], df.index.values[:100], matcher, disable=['ner', 'parser'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
