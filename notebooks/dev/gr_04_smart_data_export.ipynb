{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy\n",
    "\n",
    "from sdg_mapping.cordis import load_cordis_projects, load_cordis_project_sdgs\n",
    "from sdg_mapping.cordis.cordis_utils import FRAMEWORK_PROGRAMMES\n",
    "from sdg_mapping.utils.sdg_utils import sdg_hex_color_codes, sdg_names\n",
    "\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = []\n",
    "project_sdgs = []\n",
    "\n",
    "for fp in FRAMEWORK_PROGRAMMES:\n",
    "    projects.append(load_cordis_projects(fp).set_index('rcn'))\n",
    "    project_sdgs.append(load_cordis_project_sdgs(fp, 'probability').set_index('rcn'))\n",
    "    \n",
    "projects = pd.concat(projects)\n",
    "project_sdgs = pd.concat(project_sdgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = projects.merge(project_sdgs, left_index=True, right_index=True, how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_dir = f'{data_path}/interim/doccano/results'\n",
    "label_dir = f'{data_path}/interim/doccano/results/labels'\n",
    "\n",
    "dfs = {}\n",
    "for file in os.listdir(annotated_dir):\n",
    "    if '.csv' in file:\n",
    "        fin = os.path.join(annotated_dir, file)\n",
    "        df = pd.read_csv(fin)\n",
    "        n = int(fin.split('_')[-1].split('.')[0][3:])\n",
    "\n",
    "        label_path = os.path.join(label_dir, f'labels_sdg{n}.json')\n",
    "        labels = pd.read_json(label_path)\n",
    "        label_map = {i: k for i, k in zip(labels['id'], labels['suffix_key'])}\n",
    "        df['label'] = df['label'].map(label_map)\n",
    "        df['label'] = df['label'].map({'y': 1, 'n': 0})\n",
    "\n",
    "        df = df.rename(columns={'meta.rcn': 'rcn'})\n",
    "        df = df.set_index('rcn')\n",
    "\n",
    "        dfs[n] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs.values():\n",
    "    projects = projects.drop(projects.index.intersection(df.index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(projects[sdg_keys] > .5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = projects.sort_values(13, ascending=False)['title'].values[:20]\n",
    "for i in x:\n",
    "    print('>>>', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=4, nrows=4, figsize=(15,13))\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    projects[i+1].plot.hist(ax=ax)\n",
    "\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Documents from Across the Prediction Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_keys = list(range(1, 17))\n",
    "quintiles = [0, 20, 40, 60, 80, 100]\n",
    "\n",
    "project_ids = {}\n",
    "\n",
    "for sdg in sdg_keys:\n",
    "    probs = projects[sdg]\n",
    "    \n",
    "    steps = np.linspace(probs.min(), probs.max(), 5)\n",
    "    ids = []\n",
    "    for lower, upper in zip(steps[:-1], steps[1:]):\n",
    "        probs_step = probs[(probs > lower) & (probs <= upper)]\n",
    "        if probs_step.shape[0] < 200:\n",
    "            ids.extend(probs_step.index.values)\n",
    "        else:\n",
    "            ids_q = probs[(probs > lower) & (probs <= upper)].sample(200, random_state=0).index.values\n",
    "            ids.extend(ids_q)\n",
    "    project_ids[sdg] = ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Similar Documents that Are Not Positively Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load('/Users/grichardson/models/universal-sentence-encoder_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for chunk in chunks(projects['objective'].fillna(''), 1000):\n",
    "    embeddings.extend(embed(chunk).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AnnoyIndex(512, 'angular')\n",
    "\n",
    "for i, v in zip(projects.index.values, np.array(vec_df)):\n",
    "    t.add_item(i, v)\n",
    "    \n",
    "t.build(500)\n",
    "\n",
    "# vec_df = pd.DataFrame(np.array(embeddings))\n",
    "# vec_df.index = projects.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vecs = []\n",
    "\n",
    "for sdg in sdg_keys:\n",
    "    ids = projects.sort_values(sdg, ascending=False).index.values[:100]\n",
    "    mean_vecs.append(vec_df.loc[ids].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_ids = {}\n",
    "\n",
    "for sdg, vec in zip(sdg_keys, mean_vecs):\n",
    "    similar = t.get_nns_by_vector(vec, 3000)\n",
    "    similar_negative_projects = projects.loc[similar][projects.loc[similar][sdg] < .5][sdg]\n",
    "    similar_negative_projects = similar_negative_projects.sort_values(ascending=False)\n",
    "    \n",
    "    similar_negative_ids = set(similar_negative_projects.index.values[:200])\n",
    "    \n",
    "    sdg_ids = set(project_ids[sdg])\n",
    "    \n",
    "    extra_ids[sdg] = list(similar_negative_ids.difference(sdg_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in project_ids.items():\n",
    "    project_ids[k].extend(extra_ids[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents That are Similar to Manually Labelled Documents but Not Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs.values():\n",
    "    df['text'] = df['text'].apply(lambda x: x.split(' === ')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_labelled_vec = {}\n",
    "\n",
    "for sdg, df in dfs.items():\n",
    "    positives = df[df['label'] == 1]\n",
    "    embedding = embed(df['text'].values).numpy().mean(axis=0)\n",
    "    correctly_labelled_vec[sdg] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_ids = {}\n",
    "\n",
    "for sdg, vec in zip(sdg_keys, correctly_labelled_vec.values()):\n",
    "    similar = t.get_nns_by_vector(vec, 201)\n",
    "#     similar_negative_projects = projects.loc[similar][sdg]\n",
    "#     similar_negative_projects = similar_negative_projects.sort_values(ascending=False)\n",
    "    \n",
    "#     similar_negative_ids = set(similar_negative_projects.index.values[:200])\n",
    "    similar_negative_ids = set(similar)\n",
    "    \n",
    "    \n",
    "    sdg_ids = set(project_ids[sdg])\n",
    "    \n",
    "    extra_ids[sdg] = list(similar_negative_ids.difference(sdg_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in project_ids.items():\n",
    "    project_ids[k].extend(extra_ids[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exports = {}\n",
    "\n",
    "for sdg in sdg_keys:\n",
    "    export = projects.loc[project_ids[sdg]]\n",
    "    exports[sdg] = export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sdg in sdg_keys:\n",
    "    test = exports[sdg]\n",
    "    test['text'] = '=== ' + test['title'] + ' === ' + test['objective']\n",
    "\n",
    "    test = test.reset_index()\n",
    "    test = test[['rcn', 'text', 1]]\n",
    "    test = test.rename(columns={'rcn': 'ID', 'text': 'Text', 1: 'Label'})\n",
    "\n",
    "    test['Label'] = (test['Label'] > .5).map({True: 'Yes', False: 'No'})\n",
    "    test.dropna(inplace=True)\n",
    "    \n",
    "    test.to_csv(f'../../data/interim/smart_sdg_{sdg}.csv', index=False)\n",
    "    \n",
    "    test['Label'] = ''\n",
    "    test.to_csv(f'../../data/interim/smart_sdg_{sdg}_unlabelled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
